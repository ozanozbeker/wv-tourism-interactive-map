# Introduction

This notebooks scrapes the webpages for all of the destinations on wvtourism.com, with the exception of State Parks, Scenic Routes, and Cities as they have a far different structure or website that will be scraped in another notebook.

After studying the sitemap, I found that there are about 2,200 URLs for different destinations, websites, etc., but they are not all active. Also, the website follows a structure for grouping these links in a way I like, so I will follow the path it has set.

The website is pretty much broken down into three Groups: Things to Do, Place to Stay, & Places to Go, with Things To Do having sub-groups. Within these Groups, there are Categories of Destinations. This code will systematically extract all of these links and build our database of Destinations that we will use in this map.

# Initialize

Load the necessary packages as well as functions used across the project.

```{r}
#| label: initialize

library(tidyverse)
library(httr2)
library(rvest)
library(janitor)
library(DBI)
library(mapboxapi)

con = dbConnect(duckdb::duckdb(), "database.duckdb")
```

## Custom Functions

```{r}
#| label: custom-functions

get_request = function(url) {
  url |>
    request() |>
    req_method("GET") |>
    req_throttle(rate = 2) |> # 30 requests per minute
    req_user_agent("Ozan Ozbeker (https://github.com/ozanozbeker/wv-tourism-interactive-map)")
}

extract_categories = function(resp) {
  resp |> 
    resp_body_html() |>
    html_element("ul.ttd") |>
    html_children() |>
    html_children() |>
    html_attrs() |>
    as_tibble_col() |>
    unnest_wider(value) |>
    clean_names() |>
    filter(is.na(class)) |>
    select(category = title, url = href)
}

extract_destinations = function(resp) {

  # Metadata
  url = resp_url(resp)
  
  group = case_when(
    str_detect(url, "things-to-do") ~ "Things To Do",
    str_detect(url, "places-to-stay") ~ "Places To Stay",
    str_detect(url, "places-to-go") ~ "Places To Go",
    .default = NA_character_
  )
  
  sub_group = case_when(
    str_detect(url, "outdoor-adventures") ~ "Outdoor Adventures",
    str_detect(url, "luxury-relaxation") ~ "Luxury Relaxation",
    str_detect(url, "arts-culture-history") ~ "Arts, Culture, & History",
    .default = NA_character_
  )
  
  positions = str_locate_all(url, "/")[[1]]
  last_two = positions[(nrow(positions) - 1):nrow(positions), ]
  start = last_two[1, "end"] + 1
  end = last_two[2, "start"] - 1
  
  category = url |> str_sub(start = start, end = end)
  
  # Extract Info
  base = resp |>
    resp_body_html() |>
    html_element(".listings-wall") |>
    html_children()
  
  data_1 = base |>
    html_attrs() |>
    as_tibble_col() |>
    unnest_wider(value) |>
    clean_names() |>
    select(-class) |>
    remove_empty("rows") |>
    rename(city = data_city, region = data_region, county = data_county) |>
    mutate(across(everything(), str_to_title))
  
  data_2 = base |>
    html_children() |>
    html_children() |>
    html_elements("div.meta-top") |>
    html_children() |>
    html_attrs() |>
    as_tibble_col() |>
    unnest_wider(value) |>
    clean_names() |>
    select(title, href) |>
    mutate(
      source = if_else(str_detect(href, "wvtourism"), "wvtourism", "google"),
      title = str_remove_all(title, "LIST: |MAPS: ")) |>
    pivot_wider(
      names_from = source,
      names_prefix = "url_",
      values_from = href,
      values_fn = list) |>
    unnest_longer(col = everything()) |>
    rename(destination = title) |>
    mutate(address = str_sub(url_google, 28L, -1L)) |>
    select(-url_google)
  
  data = data_2 |> 
    bind_cols(data_1) |> 
    mutate(
      group = group,
      sub_group = sub_group,
      category = category,
      .before = destination
    )
  
  return(data)
}

pull_destination_from_url = function(url_wvtourism) {
  location_tibble = url_wvtourism |>
    str_locate_all("/") |>
    as_tibble_col() |>
    unnest("value") |>
    slice_tail(n = 2)

  last_index = location_tibble |>
    slice_tail(n = 1) |>
    pluck(1) |>
    pluck(1)

  second_last_index = location_tibble |>
    slice_head(n = 1) |>
    pluck(1) |>
    pluck(1)

  company = url_wvtourism |> str_sub(second_last_index, last_index) |>
    str_replace_all("-", " ") |>
    str_remove_all("/") |>
    str_to_title()

  return(company)
}

get_coords = function(url_wvtourism, address) {
  
  coords = mb_geocode(
    search_text = address,
    search_within = c(-82.644739, 37.201483, -77.719519, 40.638801),
    language = "en",
    country = "US"
  )
  
  data = tibble(
    url_wvtourism = url_wvtourism,
    lat = coords[[1]],
    lon = coords[[2]]
  )
  
  return(data)
}

get_address = function(url_wvtourism, lat, lon) {
  
  address = mb_reverse_geocode(
    coordinates = c(lat, lon)
  )
  
  data = tibble(
    url_wvtourism = url_wvtourism,
    address = address
  )
  
  return(data)
}

extract_page = function(resp) {
  # Read Page
  page = resp_body_html(resp)

  # Extract data
  description = page |> 
    html_element(".desc") |> 
    html_text2() |> 
    str_replace_all("\n", " ") |> 
    str_replace_all("  ", " ")
  
  url_website = page |> 
    html_element(".meta_block--website a") |> 
    html_attr("href")
  
  phone = page |> 
    html_element('span[itemprop="telephone"]') |> 
    html_text2()
  
  email = page |> 
    html_element('a[itemprop="email"]') |> 
    html_text2()

  # Return tibble
  data = tibble(
    url_wvtourism = resp_url(resp),
    description = description,
    url_website = url_website,
    phone = phone,
    email = email
  )

  return(data)
}
```

# Categories

We will use {httr2} & {rvest} to build our ultimate set of Destination urls, and start with the known URLs for the Groups.

```{r}
groups = tribble(
  ~ group, ~ url,
  "Things To Do - Outdoor Adventures", "https://wvtourism.com/things-to-do/outdoor-adventures/",
  "Things To Do - Luxury Relaxation", "https://wvtourism.com/things-to-do/luxury-relaxation/",
  "Things To Do - Arts, Culture, & History", "https://wvtourism.com/things-to-do/arts-culture-history/",
  "Places To Stay", "https://wvtourism.com/places-to-stay/",
  "Places To Go", "https://wvtourism.com/places-to-go/"
)

reqs_groups = groups |>
  pull(url) |> 
  map(get_request)

resps_groups = reqs_groups |> 
  req_perform_sequential(on_error = "continue")
```

{httr2} takes care of error-handling for us, so if something goes wrong with the HTTP request, it will document that and continue. Here we continue with the successes from `links_request` and trouble shoot the failures if necessary.

```{r}
resps_failures_groups = resps_failures(resps_groups) |> print()
resps_successes_groups = resps_successes(resps_groups) |> print()
```

Now we need to extract all of the different Categories from the Groups. We create a custom function for this that is specific to the design of wvtourism.com (at least for these pages). As mentioned in the intro, we will ignore some of the results as they will be covered in other scrapes.

```{r}
categories = resps_successes_groups |> 
  map(extract_categories) |>
  list_rbind() |> 
  filter(
    category != "Stargazing", # Section of website unfinished as of 2024-07-20
    # The following are big/different enough to need their own scripts:
    str_detect(url, "/state-parks/", negate = TRUE),    # scrape_state-parks.R
    str_detect(url, "/scenic-routes/", negate = TRUE),  # scrape_routes.R
    str_detect(url, "/travel-regions/", negate = TRUE)) # scrape_cities.R

print(categories)
```

# Destinations

With all of the Category URLs extracted, now we go back in and find all of the URLs for the featured Destinations within each Category. Will be doing the same steps as above, so I will condense all of the code to one chunk.

```{r}
reqs_categories = categories |> 
  pull(url) |>
  map(get_request)

resps_categories = reqs_categories |> 
  req_perform_sequential(on_error = "continue")

resps_failures_categories = resps_failures(resps_categories)
resps_successes_categories = resps_successes(resps_categories)

results_destinations = resps_successes_categories |> 
  map(extract_destinations, .progress = TRUE) |> 
  list_rbind()
```

Now that we have all of our destinations in one table, there is a lot of cleaning to be done before it can be used in map. There are duplicate rows for the destinations, as a destination can have more than one category. We will clean up the names, and create some normalized tables to store all of this information.

```{r}
destinations_clean = results_destinations |> 
  mutate(
    across(where(is.character), \(col) if_else(col == "", NA_character_, str_squish(col))),
    destination = coalesce(destination, pull_destination_from_url(url_wvtourism)),
    region = if_else(region == "Array", NA_character_, region),
    zip = if_else(
      str_sub(address, -5L, -5L) == "-",
      str_sub(address, -10L, -6L),
      str_sub(address, -5L, -1L)),
    across(where(is.character), \(col) if_else(col == "", NA_character_, str_squish(col))),
    category = category |> 
      str_replace_all("-", " ") |> 
      str_to_title(),
    category = case_when(
      category == "Atv Off Roading" ~ "ATV & Off-roading",
      category == "Skiing Winter Sports" ~ "Skiing & Winter Sports",
      category == "Rv And Camping" ~ "Camping & RV",
      category == "Dining Drinks" ~ "Dining & Drinks",
      category == "Farm To Table" ~ "Farm-To-Table",
      category == "History Heritage" ~ "History & Heritage",
      category == "Nightlife Entertainment" ~ "Nightlife & Entertainment",
      category == "Bbs" ~ "Bed & Breakfasts",
      category == "Hotels Motels" ~ "Hotels & Motels",
      category == "Wmas" ~ "Wildlife Management Areas",
      .default = category)) |> 
  distinct()
```

## DB Tables

Before we tidy the Destinations table even more, we can pull out some other tables we will use later.

```{r}
group_subgroup_category = destinations_clean |> 
  distinct(group, sub_group, category)

destination_category = destinations_clean |> 
  distinct(url_wvtourism, category)
```

Now that we've removed extra info, we can bring the Destinations table down to one row per URL. We will also clean up the data some more as the website is not very consistent when it comes to address and locales.

I chose to remove the address all together because the data is very poor in consistency. And when we geocode later, having the name of the business along with city and zip is more than enough.

```{r}
zips = dbReadTable(con, "zip_codes") |> as_tibble()
travel_regions = dbReadTable(con, "travel_regions") |> as_tibble()

destinations_unique = destinations_clean |> 
  select(-group, -sub_group, -address, -category, -region) |> 
  distinct() |> 
  left_join(zips, join_by(zip == zipcode)) |> 
  mutate(
    across(where(is.character), \(col) if_else(col == "", NA_character_, str_squish(col))),
    city = coalesce(major_city, city),
    county = coalesce(county.y, county.x),
    county = if_else(str_ends(county, "County"), county, str_c(county, " County")),
    state = "WV") |> 
  distinct(url_wvtourism, destination, city, county, state, zip) |> 
  drop_na(everything()) |> 
  filter(str_detect(zip, ",| |-", negate = TRUE)) |> 
  arrange(url_wvtourism)
```

## Geocoding

With the clean data tables, we need to latitude and longitude values for the destinations so that we can pin them on the map. To do this, we will use the Mapbox Geocoding API. As a bonus, this method can also get us a clean address with the reverse geocode feature.

```{r}
coords = destinations_unique |> 
  mutate(address = str_c(destination, city, state, sep = ", ")) |> 
  select(url_wvtourism, address) |> 
  pmap(get_coords, .progress = TRUE) |> 
  list_rbind()

address = coords |>
  pmap(get_address, .progress = TRUE) |> 
  list_rbind()
```

## Page Info

Finally, we will scrape the web pages for each destination on the wvtourism website. This will give us some more info like their website, phone, etc., as well as a description we can use when the user points on the icon on the map.

```{r}
reqs_pages = destinations_unique |> 
  pull(url_wvtourism) |> 
  map(get_request)

resps_pages = reqs_pages |> 
  req_perform_sequential(on_error = "continue")

resps_failures_pages = resps_failures(resps_pages)
resps_successes_pages = resps_successes(resps_pages)

results_pages = resps_successes_pages |> 
  map(extract_page, .progress = TRUE) |> 
  list_rbind()
```


# Finalize

With all of our scrapes done, we will build one final `destinations` table.

```{r}
destinations = destinations_unique |> 
  left_join(coords, join_by(url_wvtourism)) |> 
  left_join(address, join_by(url_wvtourism)) |> 
  left_join(results_pages, join_by(url_wvtourism)) |> 
  mutate(
    across(where(is.character), \(col) if_else(col == "", NA_character_, str_squish(col)))
  )
```

## Write to DB
With everything situated, we will update our database.

```{r}
dbWriteTable(con, "group_subgroup_category", group_subgroup_category)
dbWriteTable(con, "destination_category", destination_category)
dbWriteTable(con, "destinations", destinations, overwrite = TRUE)
dbListTables(con)
dbDisconnect(con)
```
