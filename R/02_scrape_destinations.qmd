# Scrape Destinations

This notebook scrapes the webpages for all of the destinations on wvtourism.com, with the exception of State Parks, Scenic Routes, and Cities as they have a different structure or website that will be scraped in another notebook.

After studying the sitemap, I found that there are around 2,200 URLs for different destinations, websites, etc., but they are not all active. Also, the website follows a structure for grouping these links in a way I like, so I will follow the path it has set.

The website is pretty much broken down into three Groups: Things to Do, Place to Stay, & Places to Go, with Things To Do having sub-groups. Within these Groups, there are Categories of Destinations. This code will extract all of these links and build our database of Destinations that we will use in this map.

## Initialize

Load the necessary packages as well as functions used across the project.

```{r}
#| label: initialize

library(tidyverse)
library(httr2)
library(rvest)
library(janitor)
library(DBI)

source("R/functions/scraping.R")

con = dbConnect(duckdb::duckdb(), "duck.db")
dbListTables(con) |> as_tibble_col("tables")
```

## Groups

Most of the destinations are split between 5 groups, the three sub groups of "Things To Do" as well as "Places To Stay" and "Places To Go".

```{r}
homepages_urls = tribble(
  ~ homepage, ~ url,
  "Outdoor Adventures", "https://wvtourism.com/things-to-do/outdoor-adventures/",
  "Luxury & Relaxation", "https://wvtourism.com/things-to-do/luxury-relaxation/",
  "Culture & Lifestyle", "https://wvtourism.com/things-to-do/arts-culture-history/",
  "Places to Stay", "https://wvtourism.com/places-to-stay/",
  "Places to Go", "https://wvtourism.com/places-to-go/"
)

homepages_reqs = homepages_urls |>
  pull(url) |> 
  map(get_request)

homepages_resps = homepages_reqs |> 
  req_perform_sequential(on_error = "continue")
```

{httr2} takes care of error-handling for us, so if something goes wrong with the HTTP request, it will document that and continue. Here we continue with the successful requests and trouble shoot the failures if necessary.

```{r}
homepages_resps_failures = resps_failures(homepages_resps) |> print()
homepages_resps_successes = resps_successes(homepages_resps) |> print()
```

Somewhere in the map UI I want to have little descriptions for the map options, so we will pull some info from the homepages.

```{r}
homepages = homepages_resps_successes |> 
  map(extract_homepage_info) |> 
  list_rbind() |> 
  rename(homepage = title) |> 
  left_join(homepages_urls, join_by(homepage))

dbWriteTable(con, "homepages", homepages)
```

## Categories

Now we need to extract all of the different Categories from the Groups. We create a custom function for this that is specific to the design of wvtourism.com (at least for these pages). As mentioned in the intro, we will ignore some of the results as they will be covered in other scrapes.

```{r}
categories_urls = homepages_resps_successes |> 
  map(extract_categories_urls) |>
  list_rbind() |> 
  filter(
    category != "Stargazing", # Section of website unfinished as of 2024-08-14
    # The following are big/different enough to need their own scripts:
    str_detect(url, "/state-parks/", negate = TRUE),    # scrape_state-parks.R
    str_detect(url, "/scenic-routes/", negate = TRUE),  # scrape_routes.R
    str_detect(url, "/travel-regions/", negate = TRUE)) # scrape_cities.R

categories_reqs = categories_urls |> pull(url) |> map(get_request)
categories_resps = categories_reqs |> req_perform_sequential(on_error = "continue")
categories_resps_failures = resps_failures(categories_resps)
categories_resps_successes = resps_successes(categories_resps)

categories = categories_resps_successes |> 
  map(extract_homepage_info) |> 
  list_rbind() |> 
  rename(category = title) |> 
  left_join(categories_urls, join_by(category))

dbWriteTable(con, "categories", categories)
```

## Destinations

With all of the Categories extracted, now we go back in and find all of the URLs for the featured Destinations within each Category. We can also pull some mapping data that we will use for later.

```{r}
destinations_urls = categories_resps_successes |> 
  map(extract_destinations_urls, .progress = TRUE) |> 
  list_rbind()

destinations_reqs = destinations_urls |> distinct(url) |> pull(url) |> map(get_request)
destinations_resps = destinations_reqs |> req_perform_sequential(on_error = "continue")
destinations_resps_successes = resps_successes(destinations_resps)
destinations_resps_failures = resps_failures(destinations_resps)
```

There is a lot of cleaning to be done before it can be used in the map. There are duplicate rows for the destinations, as a destination can have more than one category. We will clean up the names, and create some normalized tables to store all of this information.

```{r}
destination_category_cross = destinations_urls |> 
  distinct(url, category) |> 
  arrange(url)

dbWriteTable(con, "destination_category_cross", destination_category_cross)
```

Finally, we will scrape the web pages for each destination on the website. This function will be different than the usual `extract_homepage_info` we've been using because these pages are structured far differently, but will give us some more info like their website, phone, etc., as well as a description we can use when the user points on the icon on the map.

```{r}
destinations = destinations_resps_successes |> 
  map(extract_destination_info, .progress = TRUE) |> 
  list_rbind() |> 
  mutate(
    destination = coalesce(destination, pull_destination_from_url(url)),
    zip = if_else(
      str_sub(address, -5L, -5L) == "-",
      str_sub(address, -10L, -6L),
      str_sub(address, -5L, -1L)),
    across(where(is.character), \(x) if_else(x == "", NA_character_, str_squish(x)))) |> 
  distinct()

dbWriteTable(con, "destinations", destinations)
```

## Finalize

Let's look at the tables we've created so far.

```{r}
dbListTables(con) |> 
  map(\(table) dbReadTable(con, table) |> as_tibble()) |> 
  set_names(dbListTables(con))
```

Everything looks good so we will close the DB connection.

```{r}
dbDisconnect(con)
```
